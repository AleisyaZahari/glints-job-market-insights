{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KP5tDu34tk_"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3iffEC34FV8"
   },
   "outputs": [],
   "source": [
    "# --- IMPORT LIBRARY ---\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GhsV43Q53aG"
   },
   "source": [
    "# scrape company list from glints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfTHen7a5Ghi"
   },
   "outputs": [],
   "source": [
    "# === STEP 1: SCRAPE COMPANY LIST FROM GLINTS ===\n",
    "options = Options()\n",
    "options.add_argument('--start-maximized')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "company_data = []\n",
    "\n",
    "for page in range(1, 2):\n",
    "    try:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f'https://glints.com/id/companies?countries=ID&page={page}'\n",
    "        driver.get(url)\n",
    "\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[aria-label^=\"Company card\"]'))\n",
    "        )\n",
    "\n",
    "        companies = driver.find_elements(By.CSS_SELECTOR, 'a[aria-label^=\"Company card\"]')\n",
    "\n",
    "        for comp in companies:\n",
    "            try:\n",
    "                name = comp.find_element(By.CSS_SELECTOR, 'p[aria-label$=\"name\"]').text\n",
    "            except:\n",
    "                name = ''\n",
    "            try:\n",
    "                location = comp.find_element(By.CSS_SELECTOR, 'p[class*=\"LocationName\"]').text\n",
    "            except:\n",
    "                location = ''\n",
    "            try:\n",
    "                industry = comp.find_element(By.CSS_SELECTOR, 'p[class*=\"IndustryName\"]').text\n",
    "            except:\n",
    "                industry = ''\n",
    "            try:\n",
    "                inner_divs = comp.find_elements(By.CSS_SELECTOR, 'div[href]')\n",
    "                profile_link = ''\n",
    "                for div in inner_divs:\n",
    "                    profile_link = div.get_attribute('href')\n",
    "                    if profile_link:\n",
    "                        break\n",
    "            except:\n",
    "                profile_link = ''\n",
    "\n",
    "            company_data.append({\n",
    "                'Name': name,\n",
    "                'Location': location,\n",
    "                'Industry': industry,\n",
    "                'Full URL': profile_link\n",
    "            })\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal scrape page {page}: {e}\")\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(company_data)\n",
    "driver.quit()\n",
    "\n",
    "# === STEP 2: CLEAN EMPTY NAMES ===\n",
    "df = df[df['Name'] != ''].reset_index(drop=True)\n",
    "df.to_csv('glints_company_list.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Full URL'] = 'https://glints.com' + df['Full URL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ihIAEG-7C6-"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PB8kau57DO-"
   },
   "source": [
    "# pencarian addres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGOaIjJa7Eu0"
   },
   "outputs": [],
   "source": [
    "# === STEP 3: SCRAPE ADDRESS FROM COMPANY PROFILE ===\n",
    "def scrape_address(url):\n",
    "    try:\n",
    "        service = Service(r\"\")\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--start-minimized')\n",
    "        options.add_argument(\"--window-position=-32000,-32000\")\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"p\")))\n",
    "\n",
    "        all_p = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "        for i, p in enumerate(all_p):\n",
    "            if \"Alamat\" in p.text:\n",
    "                if i + 1 < len(all_p):\n",
    "                    address_text = all_p[i + 1].text\n",
    "                    driver.quit()\n",
    "                    return address_text\n",
    "\n",
    "        driver.quit()\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {url}: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wANtkl0bJIQE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Address\"] = \"\"\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notna(row['Full URL']) and (pd.isna(row['Address']) or row['Address'].strip() == \"\"):\n",
    "        print(f\"Scraping address for: {row['Name']}\")\n",
    "        address = scrape_address(row['Full URL'])\n",
    "        df.at[i, 'Address'] = address\n",
    "\n",
    "print(\"Alamat selesai dicari.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_IlrvLd7VOY"
   },
   "source": [
    "# pencarian sosmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5bmONQW7XM8"
   },
   "outputs": [],
   "source": [
    "# === STEP 4: SCRAPE SOCIAL LINKS ===\n",
    "def scrape_social_links(url):\n",
    "    try:\n",
    "        service = Service(r\"\")\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument(\"--window-position=-32000,-32000\")\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "\n",
    "        social_links = {\n",
    "            \"Website\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"X\": None\n",
    "        }\n",
    "\n",
    "        anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for a in anchors:\n",
    "            label = a.text.strip().lower()\n",
    "            href = a.get_attribute(\"href\")\n",
    "\n",
    "            if \"website\" in label:\n",
    "                social_links[\"Website\"] = href\n",
    "            elif \"instagram\" in label:\n",
    "                social_links[\"Instagram\"] = href\n",
    "            elif \"linkedin\" in label:\n",
    "                social_links[\"LinkedIn\"] = href\n",
    "            elif label in [\"x\", \"twitter\"]:\n",
    "                social_links[\"X\"] = href\n",
    "\n",
    "        driver.quit()\n",
    "        return social_links\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {url}: {e}\")\n",
    "        return {\n",
    "            \"Website\": \"NaN\",\n",
    "            \"Instagram\": \"NaN\",\n",
    "            \"LinkedIn\": \"NaN\",\n",
    "            \"X\": \"NaN\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uzGSRrW7ZrM"
   },
   "outputs": [],
   "source": [
    "df[\"Website\"] = \"\"\n",
    "df[\"Instagram\"] = \"\"\n",
    "df[\"LinkedIn\"] = \"\"\n",
    "df[\"X\"] = \"\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row[\"Full URL\"]):\n",
    "        links = scrape_social_links(row[\"Full URL\"])\n",
    "        df.at[idx, \"Website\"] = links[\"Website\"]\n",
    "        df.at[idx, \"Instagram\"] = links[\"Instagram\"]\n",
    "        df.at[idx, \"LinkedIn\"] = links[\"LinkedIn\"]\n",
    "        df.at[idx, \"X\"] = links[\"X\"]\n",
    "\n",
    "df.to_csv(\"glints_companies_with_socials.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pencarian kontak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9N89fJl7d80"
   },
   "source": [
    "##  pencarian nomor telp di yellowspage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93Nokcsv8hnE"
   },
   "source": [
    "mencari profile company dalam website yellowspage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEB1LySl7e20"
   },
   "outputs": [],
   "source": [
    "# === STEP 5: YELLOWPAGES - GET PROFILE LINK ===\n",
    "def get_yellowpages_link(nama_perusahaan, max_retries=3):\n",
    "    try:\n",
    "        for attempt in range(max_retries):\n",
    "            time.sleep(random.uniform(10, 20))\n",
    "            nama_encoded = quote(nama_perusahaan)\n",
    "            url = f\"https://www.yellowpages.id/listing/places/?q={nama_encoded}\"\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            hasil_cards = soup.find_all(\"div\", class_=\"mdc-card mdc-card--outlined\")\n",
    "            if not hasil_cards:\n",
    "                return None\n",
    "\n",
    "            for card in hasil_cards:\n",
    "                title_tag = card.find(\"h2\", class_=\"card__title mdc-typography--headline6\")\n",
    "                if title_tag:\n",
    "                    nama_ditemukan = title_tag.text.strip()\n",
    "                    similarity = SequenceMatcher(None, nama_perusahaan.lower(), nama_ditemukan.lower()).ratio()\n",
    "                    if similarity > 0.8:\n",
    "                        link_tag = title_tag.find(\"a\")\n",
    "                        if link_tag:\n",
    "                            return link_tag.get(\"href\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error yellowpages: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWD47hljJVms"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas() \n",
    "df[\"YellowPages Link\"] = df[\"Name\"].progress_apply(get_yellowpages_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLGaKJBI8qmN"
   },
   "source": [
    "### mencari nomor telp dari masing-masing link yellowspage yang tersedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uji7Pyj-8mG0"
   },
   "outputs": [],
   "source": [
    "# === STEP 6: GET PHONE NUMBER FROM YELLOWPAGES PROFILE ===\n",
    "service = Service(r\"XXXXXXXXX\")\n",
    "options = Options()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "df[\"Phone Number\"] = \"\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    link = row['YellowPages Link']\n",
    "    if pd.isna(link) or link.strip() == '':\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        phone_buttons = soup.find_all('button', class_='phone')\n",
    "\n",
    "        for button in phone_buttons:\n",
    "            number = button.get('data-number')\n",
    "            if number and number.startswith('+62'):\n",
    "                df.at[idx, 'Phone Number'] = number\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZt2zTPK-Ct2"
   },
   "source": [
    "## pencarian email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksp4PVOp87y0"
   },
   "source": [
    "### pencarian email dari website company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7: SCRAPE EMAIL & PHONE FROM WEBSITE CONTACT PAGE ===\n",
    "def extract_emails_from_website(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', html)\n",
    "            return ', '.join(set(emails)) if emails else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Terapkan ke kolom Website\n",
    "df[\"Scraped_Email\"] = df[\"Website\"].apply(extract_emails_from_website)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_clean_email_list(email_str):\n",
    "    if pd.isna(email_str):\n",
    "        return None\n",
    "    \n",
    "    # Pisahkan berdasarkan koma\n",
    "    email_list = email_str.split(',')\n",
    "\n",
    "    clean_list = []\n",
    "    for email in email_list:\n",
    "        email = email.strip().lower()\n",
    "        \n",
    "        # Cek apakah benar-benar email yang manusiawi\n",
    "        if (\n",
    "            '@' in email and\n",
    "            email.endswith(('.com', '.co.id', '.id', '.org', '.net')) and\n",
    "            not any(x in email for x in [\n",
    "                'sentry', 'your-email', 'example', '@3', '@4', '@5',\n",
    "                'sweetalert', 'core-js', 'lodash', 'bootstrap', 'jquery', 'polyfill', 'react'\n",
    "            ])\n",
    "        ):\n",
    "            clean_list.append(email)\n",
    "    \n",
    "    return ', '.join(set(clean_list)) if clean_list else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Email'] = df['Scraped_Email'].apply(filter_clean_email_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pencarian dengan serpapi\n",
    "\n",
    "API_KEY = \"XXXXXX\"\n",
    "search_url = \"https://serpapi.com/search\"\n",
    "\n",
    "# Baca data\n",
    "\n",
    "\n",
    "# Pastikan kolom baru untuk menyimpan hasil URL\n",
    "df[\"Scraped_URL\"] = df.get(\"Scraped_URL\", \"\")\n",
    "\n",
    "# Loop hanya untuk yang Cleaned_Email masih kosong\n",
    "for idx, row in df[df[\"Cleaned_Email\"].isnull()].iterrows():\n",
    "    website = row[\"Website\"]\n",
    "    if pd.isnull(website) or website.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    domain = website.replace(\"https://\", \"\").replace(\"http://\", \"\").split(\"/\")[0]\n",
    "    query = f\"site:{domain} contact OR hubungi OR kontak OR call\"\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"hl\": \"id\",\n",
    "        \"api_key\": API_KEY,\n",
    "        \"num\": 5\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, params=params, timeout=10)\n",
    "        result_json = response.json()\n",
    "\n",
    "        if \"organic_results\" in result_json:\n",
    "            for res in result_json[\"organic_results\"]:\n",
    "                link = res.get(\"link\", \"\")\n",
    "                if domain in link:\n",
    "                    df.at[idx, \"Scraped_URL\"] = link\n",
    "                    print(f\"[{idx}] {domain} ✅ {link}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"[{idx}] {domain} ❌ Tidak ada hasil relevan\")\n",
    "        else:\n",
    "            print(f\"[{idx}] {domain} ⚠️ Tidak ada hasil dari SerpAPI\")\n",
    "\n",
    "        time.sleep(1.5)  # jeda agar tidak cepat limit\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}] ⚠️ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# # Simpan hasilnya\n",
    "# df.to_csv(\"scraped_contact_url_only.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mKdjdEyMlod"
   },
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "# Pastikan kolom Scraped_Email & Scraped_Phone sudah ada\n",
    "if \"Scraped_Email\" not in df.columns:\n",
    "    df[\"Scraped_Email\"] = \"\"\n",
    "if \"Scraped_Phone\" not in df.columns:\n",
    "    df[\"Scraped_Phone\"] = \"\"\n",
    "\n",
    "# Lanjutkan hanya dari baris yang sudah memiliki Scraped_URL dan index >= 300\n",
    "contact_df = df[\n",
    "    (df.index >= start_index) &\n",
    "    df[\"Scraped_URL\"].notnull() &\n",
    "    df[\"Cleaned_Email\"].isnull() &\n",
    "    (df[\"Scraped_URL\"] != \"\")\n",
    "]\n",
    "\n",
    "for idx, row in contact_df.iterrows():\n",
    "    url = row[\"Scraped_URL\"]\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Cari semua email valid\n",
    "        emails = re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text)\n",
    "\n",
    "        # Cari nomor telepon (bisa diawali dengan +62, 62, atau 08)\n",
    "        phones = re.findall(r\"\\b(?:\\+62|62|08)[0-9]{7,13}\\b\", text)\n",
    "\n",
    "        # Bersihkan duplikat dan simpan\n",
    "        email_result = \", \".join(sorted(set(emails)))\n",
    "        phone_result = \", \".join(sorted(set(phones)))\n",
    "\n",
    "        df.at[idx, \"Scraped_Email\"] = email_result\n",
    "        df.at[idx, \"Scraped_Phone\"] = phone_result\n",
    "\n",
    "        print(f\"[{idx}] ✅ Email: {email_result} | Phone: {phone_result}\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}] ⚠️ Gagal akses {url}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_emails(text, existing_value=None):\n",
    "    # Jika sudah ada nilai di Cleaned_Email, skip (return tetap yang lama)\n",
    "    if pd.notna(existing_value) and str(existing_value).strip() != \"\":\n",
    "        return existing_value  # tidak perlu ubah\n",
    "\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return None\n",
    "\n",
    "    # Cari semua pola email valid\n",
    "    candidates = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text)\n",
    "\n",
    "    clean_list = []\n",
    "    blacklist_partial = [\n",
    "        'sentry', 'sweetalert', 'core-js', 'lodash', 'bootstrap', 'jquery', \n",
    "        'polyfill', 'react', 'popper', 'carousel', 'slick', 'cdn', 'cloudflare',\n",
    "        'placeholder', 'example', 'youremail', 'your-email', 'no-reply', 'noreply'\n",
    "    ]\n",
    "\n",
    "    for email in candidates:\n",
    "        lower_email = email.lower()\n",
    "        if not any(bad in lower_email for bad in blacklist_partial):\n",
    "            clean_list.append(lower_email)\n",
    "\n",
    "    return ', '.join(sorted(set(clean_list))) if clean_list else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cleaned_Email\"] = df.apply(\n",
    "    lambda row: extract_valid_emails(row[\"Scraped_Email\"], row[\"Cleaned_Email\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kontak email dan phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "\n",
    "# Siapkan Selenium\n",
    "service = Service(r\"\")\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Halaman turunan yang akan dicoba\n",
    "contact_paths = [\n",
    "    '', 'contact', 'contact-us', 'kontak', 'kontak-kami', 'hubungi', \n",
    "    'hubungi-kami', 'about', 'about-us', 'tentang', 'tentang-kami'\n",
    "]\n",
    "\n",
    "# Fungsi ambil email & phone\n",
    "def extract_email_phone(text):\n",
    "    emails = re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text)\n",
    "    phones = re.findall(r\"\\+?\\d[\\d\\s\\-().]{7,}\\d\", text)\n",
    "    return \", \".join(set(emails)), \", \".join(set(phones))\n",
    "\n",
    "# Scraping loop\n",
    "for index, row in df.iterrows():\n",
    "    if index < start_index:\n",
    "        continue  # Lewati sampai index yang diinginkan\n",
    "\n",
    "    if pd.notna(row['Cleaned_Email']):\n",
    "        continue  # Skip kalau sudah punya email\n",
    "\n",
    "    base_url = str(row['Website']).strip()\n",
    "    if base_url == 'nan' or base_url == '':\n",
    "        continue\n",
    "\n",
    "    found_email = \"\"\n",
    "    found_phone = \"\"\n",
    "\n",
    "    for path in contact_paths:\n",
    "        url = base_url.rstrip('/') + '/' + path.lstrip('/')\n",
    "        print(f\"[{index}] Scraping: {url}\")\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            email, phone = extract_email_phone(page_text)\n",
    "\n",
    "            if email:\n",
    "                found_email = email\n",
    "            if phone:\n",
    "                found_phone = phone\n",
    "\n",
    "            if found_email or found_phone:\n",
    "                break  # cukup satu halaman yang berhasil\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{index}] Skip {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df.at[index, 'Scraped_Email'] = found_email\n",
    "    df.at[index, 'Scraped_Phone'] = found_phone\n",
    "    \n",
    "    # df.to_excel(\"scraped_autosave.xlsx\", index=False)\n",
    "\n",
    "# Selesai\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Cleaned_Email\"] = df.apply(\n",
    "    lambda row: extract_valid_emails(row[\"Scraped_Email\"], row[\"Cleaned_Email\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"company_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"company_df.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VpfE5aB-EEk"
   },
   "source": [
    "# pencarian job search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_jobs(name, url):\n",
    "    try:\n",
    "        service = Service(r\"XXXXXXXXXXXXXXX\")\n",
    "        \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-position=10000,10000')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"jobs-section\")))\n",
    "\n",
    "        time.sleep(2)  # beri waktu render JS\n",
    "\n",
    "        job_elements = driver.find_elements(By.CSS_SELECTOR, '[aria-label^=\"Job:\"]')\n",
    "        job_data = []\n",
    "\n",
    "        for job in job_elements:\n",
    "            try:\n",
    "                title_elem = job.find_element(By.CSS_SELECTOR, 'h2 a')\n",
    "                title = title_elem.text.strip()\n",
    "                link = title_elem.get_attribute('href')\n",
    "\n",
    "                location_parts = job.find_elements(By.CSS_SELECTOR, '.CardJobLocation__LocationSpan-sc-v7ofa9-1 a')\n",
    "                location = \", \".join([l.text.strip() for l in location_parts])\n",
    "\n",
    "                job_data.append({\n",
    "                    \"Company\": name,\n",
    "                    \"Job Title\": title,\n",
    "                    \"Job Link\": link,\n",
    "                    \"Location\": location\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Gagal parsing salah satu job di {url}: {e}\")\n",
    "\n",
    "        driver.quit()\n",
    "        # return job_data\n",
    "        # ✅ Hilangkan duplikat berdasarkan Job Link\n",
    "        seen_links = set()\n",
    "        unique_data = []\n",
    "\n",
    "        for job in job_data:\n",
    "            if job[\"Job Link\"] not in seen_links:\n",
    "                unique_data.append(job)\n",
    "                seen_links.add(job[\"Job Link\"])\n",
    "\n",
    "        return unique_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat membuka {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    name = row['Name']\n",
    "    url = row['Full URL']\n",
    "    jobs = scrape_jobs(name, url)\n",
    "    all_jobs.extend(jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(all_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detail job search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_job_detail(job_url):\n",
    "    try:\n",
    "        service = Service(r\"\")\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-position=10000,10000')  # like invisible\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        driver.get(job_url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"__next\")))\n",
    "        time.sleep(2)\n",
    "\n",
    "        def safe_find(selector, multiple=False):\n",
    "            try:\n",
    "                if multiple:\n",
    "                    return driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                return driver.find_element(By.CSS_SELECTOR, selector)\n",
    "            except:\n",
    "                return [] if multiple else \"\"\n",
    "\n",
    "        container = safe_find(\".TopFoldsc__JobOverViewInfoContainer-sc-1fbktg5-8\")\n",
    "\n",
    "        # Gaji\n",
    "        salary = \"\"\n",
    "        gaji_div = safe_find(\".TopFoldsc__BasicSalary-sc-1fbktg5-13\")\n",
    "        if gaji_div:\n",
    "            salary = gaji_div.text.strip()\n",
    "\n",
    "        # Kategori & Fungsi\n",
    "        categories = safe_find(\".TopFoldsc__JobOverViewInfo-sc-1fbktg5-9.larqhx a\", multiple=True)\n",
    "        job_category = categories[0].text.strip() if len(categories) > 0 else \"\"\n",
    "        job_function = categories[1].text.strip() if len(categories) > 1 else \"\"\n",
    "\n",
    "        # Tipe kerja & lokasi kerja\n",
    "        location_divs = safe_find(\".TopFoldsc__JobOverViewInfo-sc-1fbktg5-9.larqhx\", multiple=True)\n",
    "        job_type = \"\"\n",
    "        work_place = \"\"\n",
    "        if len(location_divs) > 2:\n",
    "            texts = location_divs[2].text.split(\"·\")\n",
    "            job_type = texts[0].strip()\n",
    "            work_place = texts[1].strip() if len(texts) > 1 else \"\"\n",
    "\n",
    "        # Pendidikan & Pengalaman\n",
    "        min_edu = location_divs[3].text.strip() if len(location_divs) > 3 else \"\"\n",
    "        min_exp = location_divs[4].text.strip() if len(location_divs) > 4 else \"\"\n",
    "\n",
    "        # Tanggal\n",
    "        posted = \"\"\n",
    "        updated = \"\"\n",
    "        time_section = safe_find(\".TopFoldsc__JobOverViewTime-sc-1fbktg5-11\")\n",
    "        if time_section:\n",
    "            spans = time_section.find_elements(By.TAG_NAME, \"span\")\n",
    "            if len(spans) >= 2:\n",
    "                posted = spans[0].text.strip()\n",
    "                updated = spans[1].text.strip()\n",
    "\n",
    "        # Persyaratan\n",
    "        req_tags = safe_find(\".JobRequirementssc__Tag-sc-15g5po6-3\", multiple=True)\n",
    "        requirements = \", \".join([t.text.strip() for t in req_tags])\n",
    "\n",
    "        # Skills\n",
    "        skill_tags = safe_find(\".SkillsLegacysc__TagOverride-sc-qq2t1c-3\", multiple=True)\n",
    "        skills = \", \".join([s.text.strip() for s in skill_tags])\n",
    "\n",
    "        # Deskripsi\n",
    "        desc_div = safe_find('.DraftjsReadersc__ContentContainer-sc-zm0o3p-0')\n",
    "        job_desc = desc_div.text.strip() if desc_div else \"\"\n",
    "\n",
    "        # Timestamp scrape\n",
    "        timestamp_scraped = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"Job Category\": job_category,\n",
    "            \"Job Function\": job_function,\n",
    "            \"Employment Type\": job_type,\n",
    "            \"Work Place\": work_place,\n",
    "            \"Min Education\": min_edu,\n",
    "            \"Min Experience\": min_exp,\n",
    "            \"Salary Range\": salary,\n",
    "            \"Job Posted\": posted,\n",
    "            \"Last Updated\": updated,\n",
    "            \"Persyaratan\": requirements,\n",
    "            \"Skills\": skills,\n",
    "            \"Deskripsi\": job_desc,\n",
    "            \"Timestamp_Scraped\": timestamp_scraped,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {job_url}: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in tqdm(jobs_df.iterrows(), total=len(jobs_df)):  # ✅ pakai jobs_df\n",
    "    job_url = row['Job Link']\n",
    "    print(f\"Scraping ({i+1}/{len(jobs_df)}): {job_url}\")\n",
    "    detail = scrape_job_detail(job_url)\n",
    "    detail['Job Link'] = job_url\n",
    "    results.append(detail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details_df = pd.DataFrame(results)\n",
    "\n",
    "final_df_jobs = pd.merge(jobs_df, job_details_df, on=\"Job Link\", how=\"left\")\n",
    "\n",
    "# Simpan ke file\n",
    "final_df_jobs.to_csv(\"job_posts_with_details.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
